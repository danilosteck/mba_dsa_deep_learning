init_params <- update_params
cost_history <- c(cost_history, cost)
if (i %% 10000 == 0) cat("Iteration", i, " | Cost: ", cost, "\n")
}
model_out <- list("updated_params" = update_params,
"cost_hist" = cost_history)
return (model_out)
}
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
plot(train_model$cost_hist)
trainModel <- function(X, y, num_iteration, hidden_neurons, lr){
layer_size <- getLayerSize(X, y, hidden_neurons)
init_params <- initializeParameters(X, layer_size)
cost_history <- c()
fwd_prop <- forwardPropagation(X, init_params, layer_size)
cost <- computeCost(y, fwd_prop)
back_prop <- backwardPropagation(X, y, fwd_prop, init_params, layer_size)
update_params <- updateParameters(back_prop, init_params, learning_rate = lr)
init_params <- update_params
cost_history <- c(cost_history, cost)
#if (i %% 10000 == 0) cat("Iteration", i, " | Cost: ", cost, "\n")
model_out <- list("updated_params" = update_params,
"cost_hist" = cost_history)
return (model_out)
}
EPOCHS = 1000
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
plot(train_model$cost_hist)
trainModel <- function(X, y, num_iteration, hidden_neurons, lr){
layer_size <- getLayerSize(X, y, hidden_neurons)
init_params <- initializeParameters(X, layer_size)
cost_history <- c()
for (i in 1:num_iteration) {
fwd_prop <- forwardPropagation(X, init_params, layer_size)
cost <- computeCost(y, fwd_prop)
back_prop <- backwardPropagation(X, y, fwd_prop, init_params, layer_size)
update_params <- updateParameters(back_prop, init_params, learning_rate = lr)
init_params <- update_params
cost_history <- c(cost_history, cost)
#if (i %% 10000 == 0) cat("Iteration", i, " | Cost: ", cost, "\n")
}
model_out <- list("updated_params" = update_params,
"cost_hist" = cost_history)
return (model_out)
}
EPOCHS = 1000
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
plot(train_model$cost_hist)
cost_history <- c()
trainModel <- function(X, y, num_iteration, hidden_neurons, lr){
layer_size <- getLayerSize(X, y, hidden_neurons)
init_params <- initializeParameters(X, layer_size)
cost_history <- c()
for (i in 1:num_iteration) {
fwd_prop <- forwardPropagation(X, init_params, layer_size)
cost <- computeCost(y, fwd_prop)
back_prop <- backwardPropagation(X, y, fwd_prop, init_params, layer_size)
update_params <- updateParameters(back_prop, init_params, learning_rate = lr)
init_params <- update_params
cost_history <- c(cost_history, cost)
}
model_out <- list("updated_params" = update_params,
"cost_hist" = cost_history)
return (model_out)
}
EPOCHS = 1000
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
View(train_model)
#Gera previsões
layer_size <- getLayerSize(test_x, test_y, hidden_neurons)
#Gera previsões
layer_size <- getLayerSize(test_x, test_y, HIDDEN_NEURONS)
params <- train_model$updated_params
fwd_prop <- forwardPropagation(test_x, params, layer_size)
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)
View(compare)
#Verifica função custo
plot(train_model$cost_hist)
set.seed(0)
#Lê nossos dados de vinho
data <- read.csv(file = "winequality-red.csv")
data <- scale(data)
#Train Test Split
train_test_split_index <- 0.8 * nrow(data)
train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
#Padronizar dados para melhor performance
train_x <- data.frame(train[1:11])
train_y <- data.frame(train[12])
test_x <- data.frame(test[1:11])
test_y <- data.frame(test[12])
#transposição da matriz para facilitar
train_x <- t(train_x)
train_y <- t(train_y)
test_x <- t(test_x )
test_y <- t(test_y)
getLayerSize <- function(X, y, hidden_neurons) {
n_x <- dim(X)[1] #quantidade de linhas de x = neurônios da camada de entrada
n_h <- hidden_neurons #quantidade de neurônios na camada escondida
n_y <- dim(y)[1] #quantidade de linhas de y = neurônios da camada de saída
size <- list("n_x" = n_x,
"n_h" = n_h,
"n_y" = n_y)
return(size)
}
layer_size <- getLayerSize(train_x, train_y, hidden_neurons = 4)
layer_size
initializeParameters <- function(X, layer_size){
n_x <- layer_size$n_x
n_h <- layer_size$n_h
n_y <- layer_size$n_y
W1 <- matrix(runif(n_h * n_x), nrow = n_h, ncol = n_x, byrow = TRUE) * 0.01
W2 <- matrix(runif(n_y * n_h), nrow = n_y, ncol = n_h, byrow = TRUE) * 0.01
params <- list("W1" = W1,
"W2" = W2)
return (params)
}
init_params <- initializeParameters(train_x, layer_size)
lapply(init_params, function(x) dim(x))
sigmoid <- function(x){
return(1 / (1 + exp(-x)))
}
forwardPropagation <- function(X, params, layer_size){
n_h <- layer_size$n_h
n_y <- layer_size$n_y
W1 <- params$W1
W2 <- params$W2
Z1 <- W1 %*% X
A1 <- sigmoid(Z1)
Z2 <- W2 %*% A1
A2 <- sigmoid(Z2)
cache <- list("Z1" = Z1,
"A1" = A1,
"Z2" = Z2,
"A2" = A2)
return (cache)
}
fwd_prop <- forwardPropagation(train_x, init_params, layer_size)
computeCost <- function(y, cache) {
m <- dim(y)[2]
A2 <- cache$A2
cost <- sum((y-A2)^2)/m
return (cost)
}
cost <- computeCost(train_x, train_y, fwd_prop)
fwd_prop <- forwardPropagation(train_x, init_params, layer_size)
cost <- computeCost(train_x, train_y, fwd_prop)
cost <- computeCost(train_y, fwd_prop)
backwardPropagation <- function(X, y, cache, params, layer_size){
m <- dim(X)[2]
n_x <- layer_size$n_x
n_h <- layer_size$n_h
n_y <- layer_size$n_y
A2 <- cache$A2
A1 <- cache$A1
W2 <- params$W2
dZ2 <- A2 - y
dW2 <- 1/m * (dZ2 %*% t(A1))
dZ1 <- (t(W2) %*% dZ2) * (1 - A1^2)
dW1 <- 1/m * (dZ1 %*% t(X))
grads <- list("dW1" = dW1,
"dW2" = dW2)
return(grads)
}
updateParameters <- function(grads, params, learning_rate){
W1 <- params$W1
W2 <- params$W2
dW1 <- grads$dW1
dW2 <- grads$dW2
W1 <- W1 - learning_rate * dW1
W2 <- W2 - learning_rate * dW2
updated_params <- list("W1" = W1,
"W2" = W2)
return (updated_params)
}
trainModel <- function(X, y, num_iteration, hidden_neurons, lr){
layer_size <- getLayerSize(X, y, hidden_neurons)
init_params <- initializeParameters(X, layer_size)
cost_history <- c()
for (i in 1:num_iteration) {
fwd_prop <- forwardPropagation(X, init_params, layer_size)
cost <- computeCost(y, fwd_prop)
back_prop <- backwardPropagation(X, y, fwd_prop, init_params, layer_size)
update_params <- updateParameters(back_prop, init_params, learning_rate = lr)
init_params <- update_params
cost_history <- c(cost_history, cost)
}
model_out <- list("updated_params" = update_params,
"cost_hist" = cost_history)
return (model_out)
}
EPOCHS = 1000
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
#Gera previsões
layer_size <- getLayerSize(test_x, test_y, HIDDEN_NEURONS)
params <- train_model$updated_params
fwd_prop <- forwardPropagation(test_x, params, layer_size)
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)
#Verifica função custo
plot(train_model$cost_hist)
set.seed(0)
#Lê nossos dados de vinho
data <- read.csv(file = "winequality-red.csv")
data <- scale(data)
#Train Test Split
train_test_split_index <- 0.8 * nrow(data)
train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
#Padronizar dados para melhor performance
train_x <- data.frame(train[1:11])
train_y <- data.frame(train[12])
test_x <- data.frame(test[1:11])
test_y <- data.frame(test[12])
#transposição da matriz para facilitar
train_x <- t(train_x)
train_y <- t(train_y)
test_x <- t(test_x )
test_y <- t(test_y)
getLayerSize <- function(X, y, hidden_neurons) {
n_x <- dim(X)[1] #quantidade de linhas de x = neurônios da camada de entrada
n_h <- hidden_neurons #quantidade de neurônios na camada escondida
n_y <- dim(y)[1] #quantidade de linhas de y = neurônios da camada de saída
size <- list("n_x" = n_x,
"n_h" = n_h,
"n_y" = n_y)
return(size)
}
layer_size <- getLayerSize(train_x, train_y, hidden_neurons = 4)
layer_size
initializeParameters <- function(X, layer_size){
n_x <- layer_size$n_x
n_h <- layer_size$n_h
n_y <- layer_size$n_y
W1 <- matrix(runif(n_h * n_x), nrow = n_h, ncol = n_x, byrow = TRUE) * 0.01
W2 <- matrix(runif(n_y * n_h), nrow = n_y, ncol = n_h, byrow = TRUE) * 0.01
params <- list("W1" = W1,
"W2" = W2)
return (params)
}
init_params <- initializeParameters(train_x, layer_size)
lapply(init_params, function(x) dim(x))
sigmoid <- function(x){
return(1 / (1 + exp(-x)))
}
forwardPropagation <- function(X, params, layer_size){
n_h <- layer_size$n_h
n_y <- layer_size$n_y
W1 <- params$W1
W2 <- params$W2
Z1 <- W1 %*% X
A1 <- sigmoid(Z1)
Z2 <- W2 %*% A1
A2 <- sigmoid(Z2)
cache <- list("Z1" = Z1,
"A1" = A1,
"Z2" = Z2,
"A2" = A2)
return (cache)
}
fwd_prop <- forwardPropagation(train_x, init_params, layer_size)
computeCost <- function(y, cache) {
m <- dim(y)[2]
A2 <- cache$A2
cost <- sum((y-A2)^2)/m
return (cost)
}
cost <- computeCost(train_y, fwd_prop)
backwardPropagation <- function(X, y, cache, params, layer_size){
m <- dim(X)[2]
n_x <- layer_size$n_x
n_h <- layer_size$n_h
n_y <- layer_size$n_y
A2 <- cache$A2
A1 <- cache$A1
W2 <- params$W2
dZ2 <- A2 - y
dW2 <- 1/m * (dZ2 %*% t(A1))
dZ1 <- (t(W2) %*% dZ2) * (1 - A1^2)
dW1 <- 1/m * (dZ1 %*% t(X))
grads <- list("dW1" = dW1,
"dW2" = dW2)
return(grads)
}
updateParameters <- function(grads, params, learning_rate){
W1 <- params$W1
W2 <- params$W2
dW1 <- grads$dW1
dW2 <- grads$dW2
W1 <- W1 - learning_rate * dW1
W2 <- W2 - learning_rate * dW2
updated_params <- list("W1" = W1,
"W2" = W2)
return (updated_params)
}
trainModel <- function(X, y, num_iteration, hidden_neurons, lr){
layer_size <- getLayerSize(X, y, hidden_neurons)
init_params <- initializeParameters(X, layer_size)
cost_history <- c()
for (i in 1:num_iteration) {
fwd_prop <- forwardPropagation(X, init_params, layer_size)
cost <- computeCost(y, fwd_prop)
back_prop <- backwardPropagation(X, y, fwd_prop, init_params, layer_size)
update_params <- updateParameters(back_prop, init_params, learning_rate = lr)
init_params <- update_params
cost_history <- c(cost_history, cost)
}
model_out <- list("updated_params" = update_params,
"cost_hist" = cost_history)
return (model_out)
}
EPOCHS = 100
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
#Gera previsões
layer_size <- getLayerSize(test_x, test_y, HIDDEN_NEURONS)
params <- train_model$updated_params
fwd_prop <- forwardPropagation(test_x, params, layer_size)
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)
#Verifica função custo
plot(train_model$cost_hist)
#Lê nossos dados de vinho
data <- read.csv(file = "winequality-red.csv")
data <- scale(data)
#Lê nossos dados de vinho
data <- read.csv(file = "winequality-red.csv")
data <- scale(data)
#Train Test Split
train_test_split_index <- 0.8 * nrow(data)
train <- data.frame(data[1:train_test_split_index,])
test <- data.frame(data[(train_test_split_index+1): nrow(data),])
#Padronizar dados para melhor performance
train_x <- data.frame(train[1:11])
train_y <- data.frame(train[12])
test_x <- data.frame(test[1:11])
test_y <- data.frame(test[12])
#transposição da matriz para facilitar
train_x <- t(train_x)
train_y <- t(train_y)
test_x <- t(test_x )
test_y <- t(test_y)
getLayerSize <- function(X, y, hidden_neurons) {
n_x <- dim(X)[1] #quantidade de linhas de x = neurônios da camada de entrada
n_h <- hidden_neurons #quantidade de neurônios na camada escondida
n_y <- dim(y)[1] #quantidade de linhas de y = neurônios da camada de saída
size <- list("n_x" = n_x,
"n_h" = n_h,
"n_y" = n_y)
return(size)
}
layer_size <- getLayerSize(train_x, train_y, hidden_neurons = 4)
layer_size
initializeParameters <- function(X, layer_size){
n_x <- layer_size$n_x
n_h <- layer_size$n_h
n_y <- layer_size$n_y
W1 <- matrix(runif(n_h * n_x), nrow = n_h, ncol = n_x, byrow = TRUE) * 0.01
W2 <- matrix(runif(n_y * n_h), nrow = n_y, ncol = n_h, byrow = TRUE) * 0.01
params <- list("W1" = W1,
"W2" = W2)
return (params)
}
init_params <- initializeParameters(train_x, layer_size)
lapply(init_params, function(x) dim(x))
sigmoid <- function(x){
return(1 / (1 + exp(-x)))
}
forwardPropagation <- function(X, params, layer_size){
n_h <- layer_size$n_h
n_y <- layer_size$n_y
W1 <- params$W1
W2 <- params$W2
Z1 <- W1 %*% X
A1 <- sigmoid(Z1)
Z2 <- W2 %*% A1
A2 <- sigmoid(Z2)
cache <- list("Z1" = Z1,
"A1" = A1,
"Z2" = Z2,
"A2" = A2)
return (cache)
}
fwd_prop <- forwardPropagation(train_x, init_params, layer_size)
computeCost <- function(y, cache) {
m <- dim(y)[2]
A2 <- cache$A2
cost <- sum((y-A2)^2)/m
return (cost)
}
cost <- computeCost(train_y, fwd_prop)
backwardPropagation <- function(X, y, cache, params, layer_size){
m <- dim(X)[2]
n_x <- layer_size$n_x
n_h <- layer_size$n_h
n_y <- layer_size$n_y
A2 <- cache$A2
A1 <- cache$A1
W2 <- params$W2
dZ2 <- A2 - y
dW2 <- 1/m * (dZ2 %*% t(A1))
dZ1 <- (t(W2) %*% dZ2) * (1 - A1^2)
dW1 <- 1/m * (dZ1 %*% t(X))
grads <- list("dW1" = dW1,
"dW2" = dW2)
return(grads)
}
updateParameters <- function(grads, params, learning_rate){
W1 <- params$W1
W2 <- params$W2
dW1 <- grads$dW1
dW2 <- grads$dW2
W1 <- W1 - learning_rate * dW1
W2 <- W2 - learning_rate * dW2
updated_params <- list("W1" = W1,
"W2" = W2)
return (updated_params)
}
trainModel <- function(X, y, num_iteration, hidden_neurons, lr){
layer_size <- getLayerSize(X, y, hidden_neurons)
init_params <- initializeParameters(X, layer_size)
cost_history <- c()
for (i in 1:num_iteration) {
fwd_prop <- forwardPropagation(X, init_params, layer_size)
cost <- computeCost(y, fwd_prop)
back_prop <- backwardPropagation(X, y, fwd_prop, init_params, layer_size)
update_params <- updateParameters(back_prop, init_params, learning_rate = lr)
init_params <- update_params
cost_history <- c(cost_history, cost)
}
model_out <- list("updated_params" = update_params,
"cost_hist" = cost_history)
return (model_out)
}
EPOCHS = 100
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
#Gera previsões
layer_size <- getLayerSize(test_x, test_y, HIDDEN_NEURONS)
params <- train_model$updated_params
fwd_prop <- forwardPropagation(test_x, params, layer_size)
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)
#Verifica função custo
plot(train_model$cost_hist)
EPOCHS = 300
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
EPOCHS = 500
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
EPOCHS = 1000
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
EPOCHS = 10000
HIDDEN_NEURONS = 40
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
#Gera previsões
layer_size <- getLayerSize(test_x, test_y, HIDDEN_NEURONS)
params <- train_model$updated_params
fwd_prop <- forwardPropagation(test_x, params, layer_size)
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)
#Verifica função custo
plot(train_model$cost_hist)
compare
plot(compare[1,], compare[2,])
train
dim(train)
dim(test)
dim(train_x)
dim(train_y)
init_params <- initializeParameters(train_x, layer_size)
lapply(init_params, function(x) dim(x))
init_params <- initializeParameters(train_x, layer_size)
lapply(init_params, function(x) dim(x))
init_params <- initializeParameters(train_x, layer_size)
lapply(init_params, function(x) dim(x))
init_params <- initializeParameters(train_x, layer_size)
lapply(init_params, function(x) dim(x))
init_params <- initializeParameters(train_x, layer_size)
lapply(init_params, function(x) dim(x))
EPOCHS = 500
HIDDEN_NEURONS = 60
LEARNING_RATE = 0.01
train_model <- trainModel(train_x, train_y, hidden_neurons = HIDDEN_NEURONS, num_iteration = EPOCHS, lr = LEARNING_RATE)
#Gera previsões
layer_size <- getLayerSize(test_x, test_y, HIDDEN_NEURONS)
params <- train_model$updated_params
fwd_prop <- forwardPropagation(test_x, params, layer_size)
y_pred <- fwd_prop$A2
compare <- rbind(y_pred,test_y)
#Verifica função custo
plot(train_model$cost_hist)
table(compare)
table(compare[1,],compare[2,])
hist(compare[2,]-compare[1,])
